{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (1.8.6)\n",
      "Requirement already satisfied: torch>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (1.10.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (4.62.3)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (2021.11.1)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (0.11.0)\n",
      "Requirement already satisfied: tensorboardX>=2.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (2.5.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (1.24.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from pytorch-lightning) (4.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.26.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.6)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning) (21.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from aiohttp->fsspec[http]>2021.06.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-lightning\n",
    "!pip install tensorboard\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Tensorboard extension (for visualization purposes later)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import  transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformImg = transforms.Compose([#transforms.Resize((96,96)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda t: t / 255.)\n",
    "                               ])\n",
    "\n",
    "postprocess = transforms.Compose([\n",
    "     transforms.Lambda(lambda t: t.cpu().squeeze(0) * 255.),\n",
    "     transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH=\"anamoly_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as baseData\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "\n",
    "class FaceKeypointsData(baseData.Dataset):\n",
    "    def __init__(self,ds_path,img_sz=512):\n",
    "        super(FaceKeypointsData, self).__init__()\n",
    "        self.img_sz=img_sz\n",
    "        with open(ds_path, 'rb') as handle:\n",
    "            self.ds=pickle.load(handle)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = self.ds[\"images\"][index]\n",
    "        kps = self.ds[\"keypoints\"][index]\n",
    "        img=transformImg(img)\n",
    "        c,h,w=img.shape\n",
    "        kps=torch.tensor(kps,dtype=torch.float32)/h\n",
    "        ## reshape to nx2\n",
    "        kps=kps.reshape(15,-1)\n",
    "        return img,kps\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=FaceKeypointsData(\"../data/train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,kps=train_dataset[1]\n",
    "img.shape,kps.shape,img.min(),img.max(),kps.min(),kps.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viskps(imgs,kps):\n",
    "    import cv2\n",
    "    kps=kps.detach().cpu()\n",
    "    n,x,h,w=imgs.shape\n",
    "    visimgs=[]\n",
    "    for idx in range(n):\n",
    "        visimg=postprocess(imgs[idx])\n",
    "        visimg=cv2.cvtColor(visimg, cv2.COLOR_GRAY2RGB)\n",
    "        viskps=kps.cpu().numpy()\n",
    "        for i in range(len(viskps[idx])):\n",
    "            point=(int(viskps[idx][i][0]*h),int(viskps[idx][i][1]*h))\n",
    "            visimg=cv2.circle(visimg,point,1,(255,0,255),-1)\n",
    "        visimgs.append(torch.tensor(visimg))\n",
    "    visimgs=torch.stack(visimgs,axis=0).float()\n",
    "    return visimgs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(num):\n",
    "    imgs=torch.stack([train_dataset[i][0] for i in range(num)], dim=0)\n",
    "    kps=torch.stack([train_dataset[i][1] for i in range(num)], dim=0)\n",
    "    return (imgs,kps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
    "import cv2\n",
    "imgs,kps=get_train_data(8)\n",
    "visimgs=viskps(imgs,kps)\n",
    "#plt.imshow(visimgs[0])\n",
    "gtvisimgs=visimgs.permute(0, 3, 1, 2)\n",
    "predvisimgs=gtvisimgs\n",
    "gridimgs=torch.stack([gtvisimgs/255,predvisimgs/255],dim=1).flatten(0,1)\n",
    "print(gridimgs.shape)\n",
    "grid = torchvision.utils.make_grid(gridimgs, nrow=4, normalize=True, value_range=(0,1))\n",
    "plt.imshow(grid.permute(1,2,0))\n",
    "#imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0,1)\n",
    "#grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisCallback(pl.Callback):\n",
    "\n",
    "    def __init__(self, input_imgs,kps, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs # Images for visulaization\n",
    "        self.kps=kps\n",
    "        self.every_n_epochs = every_n_epochs # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            gtimgs=viskps(input_imgs,self.kps)\n",
    "            gtimgs=gtimgs.permute(0, 3, 1, 2)/255\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                pred_kps = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "            predimgs=viskps(input_imgs,pred_kps)\n",
    "            predimgs=predimgs.permute(0, 3, 1, 2)/255\n",
    "            # Plot and add to tensorboard\n",
    "            imgs = torch.stack([gtimgs, predimgs], dim=1).flatten(0,1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1,1))\n",
    "            trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = timm.create_model('cspresnet50',features_only=True, pretrained=True,in_chans=1,output_stride=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = encoder(torch.randn(2, 1, 96, 96))\n",
    "for x in o:\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('cspresnet50',features_only=True, pretrained=True,in_chans=1,output_stride=8)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 c_hid : int,\n",
    "                 out_points: int=15,\n",
    "                 layers: int=6,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        c_inter=c_hid//4\n",
    "        self.out_points=out_points\n",
    "\n",
    "        interModules=[]\n",
    "        for _ in range(layers):\n",
    "            interModules.append(nn.Conv2d(c_inter, c_inter, kernel_size=3, padding=1))\n",
    "            interModules.append(act_fn())\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_inter, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            *interModules,\n",
    "            nn.Conv2d(c_inter, out_channels=out_points*2, kernel_size=3, padding=1),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x=x.reshape(-1,self.out_points,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyPointModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 latent_dim: int,\n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder,\n",
    "                 num_input_channels: int = 1,\n",
    "                 width:int= 96,\n",
    "                 height:int= 96):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class()\n",
    "        self.decoder = decoder_class(latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in an image and returns the reconstructed image\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_MSE_loss(self, batch):\n",
    "        \"\"\"\n",
    "        Given a batch of images, this function returns the reconstruction loss (MSE in our case)\n",
    "        \"\"\"\n",
    "        x, _ = batch # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                         mode='min',\n",
    "                                                         factor=0.2,\n",
    "                                                         patience=20,\n",
    "                                                         min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_MSE_loss(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('test_loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KeyPointModel(1024)\n",
    "x=torch.rand(32,1,96,96)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keypoints(latent_dim):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, f\"face_kps_{latent_dim}\"),\n",
    "                         accelerator=\"cuda\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         max_epochs=300,\n",
    "                         limit_val_batches=0,\n",
    "                         num_sanity_val_steps=0,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True),\n",
    "                                    GenerateCallback(get_train_images(8), every_n_epochs=10),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, f\"face_kps_{latent_dim}.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = Autoencoder(latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    #val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    #test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    #result = {\"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keypoints(1024)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch_p38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "591ade058296000ef487104c42d37478df75296e039f312fe940d4accf2fd7b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
